{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BM20A6100 Advanced Data Analysis and Machine Learning\n",
    "## Erik Kuitunen, 0537275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:13:24.570446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731935604.731132   14862 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731935604.771270   14862 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 15:13:25.055650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"robinhood.txt\", 'rb')\n",
    "lines = []\n",
    "for line in file:\n",
    "    line = line.strip().lower()\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    \n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "    \n",
    "file.close()\n",
    "text = \" \".join(lines)\n",
    "\n",
    "# set of characters that occur in the text\n",
    "chars = set( [c for c in text] )\n",
    "\n",
    "# Total items in our vocabulary\n",
    "unique_chars = len( chars )\n",
    "\n",
    "# lookup tables to deal with indexes of characters rather than the characters themselves.\n",
    "char2index = dict( (c, i) for i, c in enumerate( chars ) )\n",
    "index2char = dict( (i, c) for i, c in enumerate( chars ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping and one-hot encoding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "step = 1\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range( 0, len(text) - sequence_length, step ):\n",
    "    input_chars.append(text[i:i + sequence_length])\n",
    "    label_chars.append(text[i + sequence_length])\n",
    "    \n",
    "X = np.zeros((len(input_chars), sequence_length, unique_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), unique_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:13:30.968620: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/eki/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 100\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, \n",
    "                    input_shape=(sequence_length, unique_chars), \n",
    "                    unroll=True))\n",
    "model.add(Dense(unique_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:13:32.436731: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 219621640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 2.1877\n",
      "Iteration #: 1\n",
      "\u001b[1m  12/3991\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - loss: 1.8089  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:13:59.267147: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 219621640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.7423\n",
      "Iteration #: 2\n",
      "\u001b[1m  21/3991\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.5686"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:14:27.553299: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 219621640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.5984\n",
      "Iteration #: 3\n",
      "\u001b[1m  10/3991\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.5815  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:14:55.764187: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 219621640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - loss: 1.5239\n",
      "Iteration #: 4\n",
      "\u001b[1m  24/3991\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 1.4510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 15:15:18.266627: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 219621640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.4727\n",
      "Iteration #: 5\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 1.4387\n",
      "Iteration #: 6\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.4162\n",
      "Iteration #: 7\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.3951\n",
      "Iteration #: 8\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.3801\n",
      "Iteration #: 9\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.3693\n",
      "Iteration #: 10\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.3553\n",
      "Iteration #: 11\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.3496\n",
      "Iteration #: 12\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.3383\n",
      "Iteration #: 13\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.3325\n",
      "Iteration #: 14\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - loss: 1.3272\n",
      "Iteration #: 15\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.3231\n",
      "Iteration #: 16\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.3190\n",
      "Iteration #: 17\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.3160\n",
      "Iteration #: 18\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.3047\n",
      "Iteration #: 19\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 1.3025\n",
      "Iteration #: 20\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.2989\n",
      "Iteration #: 21\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2979\n",
      "Iteration #: 22\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 1.2930\n",
      "Iteration #: 23\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2917\n",
      "Iteration #: 24\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2907\n",
      "Iteration #: 25\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 1.2879\n",
      "Iteration #: 26\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2858\n",
      "Iteration #: 27\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 1.2865\n",
      "Iteration #: 28\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 9ms/step - loss: 1.2860\n",
      "Iteration #: 29\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2811\n",
      "Iteration #: 30\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 11ms/step - loss: 1.2757\n",
      "Iteration #: 31\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 1.2766\n",
      "Iteration #: 32\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.2739\n",
      "Iteration #: 33\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2715\n",
      "Iteration #: 34\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2720\n",
      "Iteration #: 35\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 8ms/step - loss: 1.2707\n",
      "Iteration #: 36\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 10ms/step - loss: 1.2725\n",
      "Iteration #: 37\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 9ms/step - loss: 1.2675\n",
      "Iteration #: 38\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2662\n",
      "Iteration #: 39\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2640\n",
      "Iteration #: 40\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2655\n",
      "Iteration #: 41\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 11ms/step - loss: 1.2647\n",
      "Iteration #: 42\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8ms/step - loss: 1.2608\n",
      "Iteration #: 43\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 1.2644\n",
      "Iteration #: 44\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2590\n",
      "Iteration #: 45\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 10ms/step - loss: 1.2623\n",
      "Iteration #: 46\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 8ms/step - loss: 1.2580\n",
      "Iteration #: 47\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2601\n",
      "Iteration #: 48\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 1.2598\n",
      "Iteration #: 49\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2574\n",
      "Iteration #: 50\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2541\n",
      "Iteration #: 51\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 1.2562\n",
      "Iteration #: 52\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 9ms/step - loss: 1.2497\n",
      "Iteration #: 53\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 8ms/step - loss: 1.2568\n",
      "Iteration #: 54\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - loss: 1.2532\n",
      "Iteration #: 55\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2540\n",
      "Iteration #: 56\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2512\n",
      "Iteration #: 57\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2527\n",
      "Iteration #: 58\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2510\n",
      "Iteration #: 59\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2460\n",
      "Iteration #: 60\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2529\n",
      "Iteration #: 61\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 9ms/step - loss: 1.2499\n",
      "Iteration #: 62\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2525\n",
      "Iteration #: 63\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2517\n",
      "Iteration #: 64\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2483\n",
      "Iteration #: 65\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 1.2482\n",
      "Iteration #: 66\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - loss: 1.2493\n",
      "Iteration #: 67\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2438\n",
      "Iteration #: 68\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2465\n",
      "Iteration #: 69\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2428\n",
      "Iteration #: 70\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2449\n",
      "Iteration #: 71\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2466\n",
      "Iteration #: 72\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 11ms/step - loss: 1.2467\n",
      "Iteration #: 73\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - loss: 1.2454\n",
      "Iteration #: 74\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8ms/step - loss: 1.2458\n",
      "Iteration #: 75\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 1.2458\n",
      "Iteration #: 76\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 9ms/step - loss: 1.2446\n",
      "Iteration #: 77\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 1.2428\n",
      "Iteration #: 78\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2440\n",
      "Iteration #: 79\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - loss: 1.2455\n",
      "Iteration #: 80\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 1.2439\n",
      "Iteration #: 81\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 1.2426\n",
      "Iteration #: 82\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2388\n",
      "Iteration #: 83\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2404\n",
      "Iteration #: 84\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2437\n",
      "Iteration #: 85\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.2436\n",
      "Iteration #: 86\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - loss: 1.2445\n",
      "Iteration #: 87\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - loss: 1.2407\n",
      "Iteration #: 88\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2369\n",
      "Iteration #: 89\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.2439\n",
      "Iteration #: 90\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.2409\n",
      "Iteration #: 91\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.2412\n",
      "Iteration #: 92\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7ms/step - loss: 1.2361\n",
      "Iteration #: 93\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.2403\n",
      "Iteration #: 94\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.2404\n",
      "Iteration #: 95\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - loss: 1.2412\n",
      "Iteration #: 96\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.2396\n",
      "Iteration #: 97\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.2375\n",
      "Iteration #: 98\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 1.2364\n",
      "Iteration #: 99\n",
      "\u001b[1m3991/3991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - loss: 1.2356\n",
      "\n",
      "Generating from seed: ck close t\n",
      "ck close th\n",
      "e\n",
      " \n",
      "s\n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      " \n",
      "l\n",
      "i\n",
      "t\n",
      "t\n",
      "l\n",
      "e\n",
      " \n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "e\n",
      "n\n",
      "g\n",
      "t\n",
      "h\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "p\n",
      "i\n",
      "n\n",
      "d\n",
      "l\n",
      "e\n",
      ",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    \n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating from seed: the other;\n"
     ]
    }
   ],
   "source": [
    "test_idx = np.random.randint(len(input_chars))\n",
    "test_chars = input_chars[test_idx]\n",
    "predicted_text = test_chars\n",
    "\n",
    "print(\"\\nGenerating from seed: %s\" % (test_chars))\n",
    "# print(test_chars, end=\"\")\n",
    "\n",
    "for i in range(70):\n",
    "    Xtest = np.zeros( (1, sequence_length, unique_chars) )\n",
    "    for i, ch in enumerate(test_chars):\n",
    "        Xtest[0, i, char2index[ch]] = 1\n",
    "    pred = model.predict(Xtest, verbose=0)[0]\n",
    "    ypred = index2char[np.argmax(pred)]\n",
    "    predicted_text += ypred\n",
    "    # print(ypred, end=\"\\n\")\n",
    "    # move forward with test_chars + ypred\n",
    "    test_chars = test_chars[1:] + ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the other; the children was a tree with the sparrow was all the spot in the spin'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
