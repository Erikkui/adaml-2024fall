{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BM20A6100 Advanced Data Analysis and Machine Learning\n",
    "## Erik Kuitunen, 0537275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data, load as dataset dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f549ea31520c40cab35c960f13d2964c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "file = open(\"robinhood.txt\", 'rb')\n",
    "lines = []\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "file.close()\n",
    "\n",
    "file = open(\"robinhood_cleaned.txt\", 'w')\n",
    "for line in lines:\n",
    "    file.write(line + \"\\n\")\n",
    "file.close()\n",
    "\n",
    "datasets = load_dataset(\"text\", data_files={\"train\": \"robinhood_cleaned.txt\" } )\n",
    "\n",
    "text_all = \" \".join( datasets[\"train\"][\"text\"] )\n",
    "words = text_all.split()\n",
    "\n",
    "# set of characters that occur in the text\n",
    "chars = set( [c for c in text_all] )\n",
    "\n",
    "# Total items in our vocabulary\n",
    "unique_chars = len( chars )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'THE MERRY ADVENTURES OF ROBIN HOOD'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a tokenizer using Byte-Pair Encoding (BPE) tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer( models.BPE() )\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel( add_prefix_space=False )    # add_prefix_space: whether to add a space before the first word \"hello\" or \" hello\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the tokenizer and adding post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trainer = trainers.BpeTrainer( vocab_size=unique_chars )    # Using same vocab size as last week for better comparison\n",
    "tokenizer.train_from_iterator( text_all, trainer=tokenizer_trainer )\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping our own tokenizer into GPT-2 tokenizer object. We will be using GPT-2 architecture also model in training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "robinhood_tokenizer = GPT2TokenizerFast( tokenizer_object=tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb010a930651425aa743775a8c1775d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return robinhood_tokenizer( examples[\"text\"] )\n",
    "\n",
    "tokenized_datasets = datasets.map( tokenize_function, remove_columns=[\"text\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping text; I do not wholly understand everything this function does. I understand it combines the tokenized data and corresponding output (\"X\", and \"y\") for the model, in addition to splitting the data into equal sized chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decdea26aec54ff8b3a118defae23883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    block_size = 128        # Size of each input block, may be adjusted. Has big impact on training time.\n",
    "    \n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model (GPT-2 architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.from_pretrained( \"gpt2\" )\n",
    "model = AutoModelForCausalLM.from_config( config )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating trainer and its arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs = 200\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbf1167458f4e52a9adc12c8685de1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2439, 'grad_norm': 2.126720428466797, 'learning_rate': 1.991023339317774e-05, 'epoch': 0.9}\n",
      "{'loss': 2.2916, 'grad_norm': 2.686671018600464, 'learning_rate': 1.9820466786355476e-05, 'epoch': 1.8}\n",
      "{'loss': 2.0704, 'grad_norm': 3.501823663711548, 'learning_rate': 1.9730700179533215e-05, 'epoch': 2.69}\n",
      "{'loss': 1.9011, 'grad_norm': 3.5388927459716797, 'learning_rate': 1.9640933572710953e-05, 'epoch': 3.59}\n",
      "{'loss': 1.7795, 'grad_norm': 3.3054778575897217, 'learning_rate': 1.955116696588869e-05, 'epoch': 4.49}\n",
      "{'loss': 1.688, 'grad_norm': 3.7335259914398193, 'learning_rate': 1.9461400359066428e-05, 'epoch': 5.39}\n",
      "{'loss': 1.616, 'grad_norm': 3.553478479385376, 'learning_rate': 1.9371633752244166e-05, 'epoch': 6.28}\n",
      "{'loss': 1.5593, 'grad_norm': 3.7092859745025635, 'learning_rate': 1.9281867145421905e-05, 'epoch': 7.18}\n",
      "{'loss': 1.5077, 'grad_norm': 3.804635763168335, 'learning_rate': 1.9192100538599644e-05, 'epoch': 8.08}\n",
      "{'loss': 1.4592, 'grad_norm': 3.8332812786102295, 'learning_rate': 1.910233393177738e-05, 'epoch': 8.98}\n",
      "{'loss': 1.4171, 'grad_norm': 3.86856746673584, 'learning_rate': 1.9012567324955118e-05, 'epoch': 9.87}\n",
      "{'loss': 1.3793, 'grad_norm': 3.805182695388794, 'learning_rate': 1.8922800718132857e-05, 'epoch': 10.77}\n",
      "{'loss': 1.3432, 'grad_norm': 4.106681823730469, 'learning_rate': 1.8833034111310592e-05, 'epoch': 11.67}\n",
      "{'loss': 1.3097, 'grad_norm': 4.286151885986328, 'learning_rate': 1.874326750448833e-05, 'epoch': 12.57}\n",
      "{'loss': 1.2712, 'grad_norm': 4.485691070556641, 'learning_rate': 1.865350089766607e-05, 'epoch': 13.46}\n",
      "{'loss': 1.2399, 'grad_norm': 4.375682353973389, 'learning_rate': 1.8563734290843805e-05, 'epoch': 14.36}\n",
      "{'loss': 1.2096, 'grad_norm': 5.373925685882568, 'learning_rate': 1.8473967684021544e-05, 'epoch': 15.26}\n",
      "{'loss': 1.1731, 'grad_norm': 5.510181903839111, 'learning_rate': 1.8384201077199283e-05, 'epoch': 16.16}\n",
      "{'loss': 1.1415, 'grad_norm': 5.194092273712158, 'learning_rate': 1.829443447037702e-05, 'epoch': 17.06}\n",
      "{'loss': 1.1035, 'grad_norm': 5.401666164398193, 'learning_rate': 1.820466786355476e-05, 'epoch': 17.95}\n",
      "{'loss': 1.063, 'grad_norm': 5.721475124359131, 'learning_rate': 1.81149012567325e-05, 'epoch': 18.85}\n",
      "{'loss': 1.0233, 'grad_norm': 6.031516075134277, 'learning_rate': 1.8025134649910235e-05, 'epoch': 19.75}\n",
      "{'loss': 0.9916, 'grad_norm': 6.6887712478637695, 'learning_rate': 1.7935368043087973e-05, 'epoch': 20.65}\n",
      "{'loss': 0.9526, 'grad_norm': 6.142903804779053, 'learning_rate': 1.7845601436265712e-05, 'epoch': 21.54}\n",
      "{'loss': 0.9109, 'grad_norm': 6.927117347717285, 'learning_rate': 1.7755834829443448e-05, 'epoch': 22.44}\n",
      "{'loss': 0.8764, 'grad_norm': 7.546774864196777, 'learning_rate': 1.7666068222621186e-05, 'epoch': 23.34}\n",
      "{'loss': 0.8383, 'grad_norm': 7.467527866363525, 'learning_rate': 1.7576301615798925e-05, 'epoch': 24.24}\n",
      "{'loss': 0.8023, 'grad_norm': 7.889253616333008, 'learning_rate': 1.748653500897666e-05, 'epoch': 25.13}\n",
      "{'loss': 0.7651, 'grad_norm': 7.836690902709961, 'learning_rate': 1.73967684021544e-05, 'epoch': 26.03}\n",
      "{'loss': 0.7185, 'grad_norm': 8.069256782531738, 'learning_rate': 1.7307001795332138e-05, 'epoch': 26.93}\n",
      "{'loss': 0.6788, 'grad_norm': 8.45073127746582, 'learning_rate': 1.7217235188509877e-05, 'epoch': 27.83}\n",
      "{'loss': 0.6406, 'grad_norm': 9.612942695617676, 'learning_rate': 1.7127468581687616e-05, 'epoch': 28.73}\n",
      "{'loss': 0.6069, 'grad_norm': 8.778581619262695, 'learning_rate': 1.703770197486535e-05, 'epoch': 29.62}\n",
      "{'loss': 0.5737, 'grad_norm': 9.462711334228516, 'learning_rate': 1.694793536804309e-05, 'epoch': 30.52}\n",
      "{'loss': 0.5429, 'grad_norm': 8.7034912109375, 'learning_rate': 1.685816876122083e-05, 'epoch': 31.42}\n",
      "{'loss': 0.5076, 'grad_norm': 9.58139419555664, 'learning_rate': 1.6768402154398564e-05, 'epoch': 32.32}\n",
      "{'loss': 0.4818, 'grad_norm': 9.21778678894043, 'learning_rate': 1.6678635547576303e-05, 'epoch': 33.21}\n",
      "{'loss': 0.4554, 'grad_norm': 9.52592945098877, 'learning_rate': 1.658886894075404e-05, 'epoch': 34.11}\n",
      "{'loss': 0.4275, 'grad_norm': 8.11138916015625, 'learning_rate': 1.6499102333931777e-05, 'epoch': 35.01}\n",
      "{'loss': 0.3958, 'grad_norm': 9.78531265258789, 'learning_rate': 1.6409335727109516e-05, 'epoch': 35.91}\n",
      "{'loss': 0.3743, 'grad_norm': 9.549946784973145, 'learning_rate': 1.6319569120287254e-05, 'epoch': 36.8}\n",
      "{'loss': 0.3524, 'grad_norm': 8.99019718170166, 'learning_rate': 1.6229802513464993e-05, 'epoch': 37.7}\n",
      "{'loss': 0.3334, 'grad_norm': 8.858589172363281, 'learning_rate': 1.614003590664273e-05, 'epoch': 38.6}\n",
      "{'loss': 0.3166, 'grad_norm': 8.608098983764648, 'learning_rate': 1.6050269299820467e-05, 'epoch': 39.5}\n",
      "{'loss': 0.3018, 'grad_norm': 8.670233726501465, 'learning_rate': 1.5960502692998206e-05, 'epoch': 40.39}\n",
      "{'loss': 0.2858, 'grad_norm': 9.155657768249512, 'learning_rate': 1.5870736086175945e-05, 'epoch': 41.29}\n",
      "{'loss': 0.2724, 'grad_norm': 8.234882354736328, 'learning_rate': 1.5780969479353684e-05, 'epoch': 42.19}\n",
      "{'loss': 0.2587, 'grad_norm': 7.561951160430908, 'learning_rate': 1.569120287253142e-05, 'epoch': 43.09}\n",
      "{'loss': 0.2471, 'grad_norm': 9.54740047454834, 'learning_rate': 1.5601436265709158e-05, 'epoch': 43.99}\n",
      "{'loss': 0.2341, 'grad_norm': 9.195537567138672, 'learning_rate': 1.5511669658886893e-05, 'epoch': 44.88}\n",
      "{'loss': 0.2242, 'grad_norm': 7.838511943817139, 'learning_rate': 1.5421903052064632e-05, 'epoch': 45.78}\n",
      "{'loss': 0.2145, 'grad_norm': 8.66828727722168, 'learning_rate': 1.533213644524237e-05, 'epoch': 46.68}\n",
      "{'loss': 0.2072, 'grad_norm': 8.343426704406738, 'learning_rate': 1.5242369838420108e-05, 'epoch': 47.58}\n",
      "{'loss': 0.2, 'grad_norm': 7.940240859985352, 'learning_rate': 1.5152603231597847e-05, 'epoch': 48.47}\n",
      "{'loss': 0.1925, 'grad_norm': 7.8363494873046875, 'learning_rate': 1.5062836624775586e-05, 'epoch': 49.37}\n",
      "{'loss': 0.1861, 'grad_norm': 6.658910274505615, 'learning_rate': 1.4973070017953321e-05, 'epoch': 50.27}\n",
      "{'loss': 0.1808, 'grad_norm': 8.302066802978516, 'learning_rate': 1.488330341113106e-05, 'epoch': 51.17}\n",
      "{'loss': 0.1755, 'grad_norm': 7.2515411376953125, 'learning_rate': 1.4793536804308799e-05, 'epoch': 52.06}\n",
      "{'loss': 0.1701, 'grad_norm': 8.152660369873047, 'learning_rate': 1.4703770197486536e-05, 'epoch': 52.96}\n",
      "{'loss': 0.1619, 'grad_norm': 8.065359115600586, 'learning_rate': 1.4614003590664274e-05, 'epoch': 53.86}\n",
      "{'loss': 0.1573, 'grad_norm': 8.051106452941895, 'learning_rate': 1.4524236983842013e-05, 'epoch': 54.76}\n",
      "{'loss': 0.1538, 'grad_norm': 8.389232635498047, 'learning_rate': 1.4434470377019749e-05, 'epoch': 55.66}\n",
      "{'loss': 0.1498, 'grad_norm': 6.517719268798828, 'learning_rate': 1.4344703770197487e-05, 'epoch': 56.55}\n",
      "{'loss': 0.1463, 'grad_norm': 6.299910545349121, 'learning_rate': 1.4254937163375226e-05, 'epoch': 57.45}\n",
      "{'loss': 0.1428, 'grad_norm': 7.016315937042236, 'learning_rate': 1.4165170556552963e-05, 'epoch': 58.35}\n",
      "{'loss': 0.14, 'grad_norm': 6.296853065490723, 'learning_rate': 1.4075403949730702e-05, 'epoch': 59.25}\n",
      "{'loss': 0.1368, 'grad_norm': 6.8823676109313965, 'learning_rate': 1.398563734290844e-05, 'epoch': 60.14}\n",
      "{'loss': 0.1344, 'grad_norm': 5.948953151702881, 'learning_rate': 1.3895870736086176e-05, 'epoch': 61.04}\n",
      "{'loss': 0.1305, 'grad_norm': 6.697444915771484, 'learning_rate': 1.3806104129263915e-05, 'epoch': 61.94}\n",
      "{'loss': 0.1278, 'grad_norm': 6.758430004119873, 'learning_rate': 1.3716337522441652e-05, 'epoch': 62.84}\n",
      "{'loss': 0.1255, 'grad_norm': 6.673800945281982, 'learning_rate': 1.3626570915619391e-05, 'epoch': 63.73}\n",
      "{'loss': 0.122, 'grad_norm': 6.8968000411987305, 'learning_rate': 1.353680430879713e-05, 'epoch': 64.63}\n",
      "{'loss': 0.1204, 'grad_norm': 6.146773338317871, 'learning_rate': 1.3447037701974865e-05, 'epoch': 65.53}\n",
      "{'loss': 0.119, 'grad_norm': 6.528747081756592, 'learning_rate': 1.3357271095152604e-05, 'epoch': 66.43}\n",
      "{'loss': 0.1166, 'grad_norm': 5.639066219329834, 'learning_rate': 1.3267504488330343e-05, 'epoch': 67.32}\n",
      "{'loss': 0.1152, 'grad_norm': 6.90316915512085, 'learning_rate': 1.317773788150808e-05, 'epoch': 68.22}\n",
      "{'loss': 0.1129, 'grad_norm': 4.778124809265137, 'learning_rate': 1.3087971274685819e-05, 'epoch': 69.12}\n",
      "{'loss': 0.1117, 'grad_norm': 5.911223411560059, 'learning_rate': 1.2998204667863557e-05, 'epoch': 70.02}\n",
      "{'loss': 0.1087, 'grad_norm': 5.798797130584717, 'learning_rate': 1.2908438061041293e-05, 'epoch': 70.92}\n",
      "{'loss': 0.1059, 'grad_norm': 6.095417499542236, 'learning_rate': 1.2818671454219031e-05, 'epoch': 71.81}\n",
      "{'loss': 0.1052, 'grad_norm': 6.2041544914245605, 'learning_rate': 1.272890484739677e-05, 'epoch': 72.71}\n",
      "{'loss': 0.105, 'grad_norm': 6.711723327636719, 'learning_rate': 1.2639138240574507e-05, 'epoch': 73.61}\n",
      "{'loss': 0.1024, 'grad_norm': 5.157321929931641, 'learning_rate': 1.2549371633752246e-05, 'epoch': 74.51}\n",
      "{'loss': 0.1012, 'grad_norm': 6.009546756744385, 'learning_rate': 1.2459605026929983e-05, 'epoch': 75.4}\n",
      "{'loss': 0.0995, 'grad_norm': 6.479554653167725, 'learning_rate': 1.236983842010772e-05, 'epoch': 76.3}\n",
      "{'loss': 0.0981, 'grad_norm': 6.452102184295654, 'learning_rate': 1.2280071813285459e-05, 'epoch': 77.2}\n",
      "{'loss': 0.0978, 'grad_norm': 5.812020778656006, 'learning_rate': 1.2190305206463198e-05, 'epoch': 78.1}\n",
      "{'loss': 0.0965, 'grad_norm': 5.401928901672363, 'learning_rate': 1.2100538599640935e-05, 'epoch': 78.99}\n",
      "{'loss': 0.0939, 'grad_norm': 6.009148120880127, 'learning_rate': 1.2010771992818672e-05, 'epoch': 79.89}\n",
      "{'loss': 0.0934, 'grad_norm': 6.635604381561279, 'learning_rate': 1.1921005385996409e-05, 'epoch': 80.79}\n",
      "{'loss': 0.092, 'grad_norm': 4.679248332977295, 'learning_rate': 1.1831238779174148e-05, 'epoch': 81.69}\n",
      "{'loss': 0.0919, 'grad_norm': 5.48586368560791, 'learning_rate': 1.1741472172351887e-05, 'epoch': 82.59}\n",
      "{'loss': 0.0898, 'grad_norm': 6.310047626495361, 'learning_rate': 1.1651705565529622e-05, 'epoch': 83.48}\n",
      "{'loss': 0.0898, 'grad_norm': 5.7202911376953125, 'learning_rate': 1.1561938958707361e-05, 'epoch': 84.38}\n",
      "{'loss': 0.0881, 'grad_norm': 5.600395679473877, 'learning_rate': 1.14721723518851e-05, 'epoch': 85.28}\n",
      "{'loss': 0.0874, 'grad_norm': 4.953324317932129, 'learning_rate': 1.1382405745062837e-05, 'epoch': 86.18}\n",
      "{'loss': 0.0866, 'grad_norm': 5.4644999504089355, 'learning_rate': 1.1292639138240576e-05, 'epoch': 87.07}\n",
      "{'loss': 0.0851, 'grad_norm': 5.627583026885986, 'learning_rate': 1.1202872531418314e-05, 'epoch': 87.97}\n",
      "{'loss': 0.0851, 'grad_norm': 3.6554553508758545, 'learning_rate': 1.111310592459605e-05, 'epoch': 88.87}\n",
      "{'loss': 0.0835, 'grad_norm': 4.408438682556152, 'learning_rate': 1.1023339317773789e-05, 'epoch': 89.77}\n",
      "{'loss': 0.0838, 'grad_norm': 4.6590447425842285, 'learning_rate': 1.0933572710951527e-05, 'epoch': 90.66}\n",
      "{'loss': 0.082, 'grad_norm': 5.454280376434326, 'learning_rate': 1.0843806104129264e-05, 'epoch': 91.56}\n",
      "{'loss': 0.0812, 'grad_norm': 4.1163010597229, 'learning_rate': 1.0754039497307003e-05, 'epoch': 92.46}\n",
      "{'loss': 0.0805, 'grad_norm': 4.736639976501465, 'learning_rate': 1.0664272890484742e-05, 'epoch': 93.36}\n",
      "{'loss': 0.0798, 'grad_norm': 4.615954399108887, 'learning_rate': 1.0574506283662477e-05, 'epoch': 94.25}\n",
      "{'loss': 0.0795, 'grad_norm': 4.606914520263672, 'learning_rate': 1.0484739676840216e-05, 'epoch': 95.15}\n",
      "{'loss': 0.0786, 'grad_norm': 5.473309516906738, 'learning_rate': 1.0394973070017955e-05, 'epoch': 96.05}\n",
      "{'loss': 0.0781, 'grad_norm': 4.141443252563477, 'learning_rate': 1.0305206463195692e-05, 'epoch': 96.95}\n",
      "{'loss': 0.0771, 'grad_norm': 4.682273864746094, 'learning_rate': 1.021543985637343e-05, 'epoch': 97.85}\n",
      "{'loss': 0.0765, 'grad_norm': 4.727480411529541, 'learning_rate': 1.0125673249551166e-05, 'epoch': 98.74}\n",
      "{'loss': 0.076, 'grad_norm': 5.459367752075195, 'learning_rate': 1.0035906642728905e-05, 'epoch': 99.64}\n",
      "{'loss': 0.0757, 'grad_norm': 4.221061706542969, 'learning_rate': 9.946140035906644e-06, 'epoch': 100.54}\n",
      "{'loss': 0.0752, 'grad_norm': 4.393372058868408, 'learning_rate': 9.85637342908438e-06, 'epoch': 101.44}\n",
      "{'loss': 0.0738, 'grad_norm': 4.524268627166748, 'learning_rate': 9.76660682226212e-06, 'epoch': 102.33}\n",
      "{'loss': 0.0742, 'grad_norm': 4.176281452178955, 'learning_rate': 9.676840215439857e-06, 'epoch': 103.23}\n",
      "{'loss': 0.0734, 'grad_norm': 3.980114459991455, 'learning_rate': 9.587073608617595e-06, 'epoch': 104.13}\n",
      "{'loss': 0.073, 'grad_norm': 4.02183723449707, 'learning_rate': 9.497307001795333e-06, 'epoch': 105.03}\n",
      "{'loss': 0.0715, 'grad_norm': 4.472980499267578, 'learning_rate': 9.40754039497307e-06, 'epoch': 105.92}\n",
      "{'loss': 0.071, 'grad_norm': 4.2840657234191895, 'learning_rate': 9.317773788150808e-06, 'epoch': 106.82}\n",
      "{'loss': 0.0705, 'grad_norm': 5.3801140785217285, 'learning_rate': 9.228007181328547e-06, 'epoch': 107.72}\n",
      "{'loss': 0.0703, 'grad_norm': 2.935469388961792, 'learning_rate': 9.138240574506284e-06, 'epoch': 108.62}\n",
      "{'loss': 0.0694, 'grad_norm': 4.235132694244385, 'learning_rate': 9.048473967684023e-06, 'epoch': 109.52}\n",
      "{'loss': 0.0693, 'grad_norm': 3.93692946434021, 'learning_rate': 8.95870736086176e-06, 'epoch': 110.41}\n",
      "{'loss': 0.0687, 'grad_norm': 2.047797918319702, 'learning_rate': 8.868940754039497e-06, 'epoch': 111.31}\n",
      "{'loss': 0.0687, 'grad_norm': 3.9615514278411865, 'learning_rate': 8.779174147217236e-06, 'epoch': 112.21}\n",
      "{'loss': 0.0681, 'grad_norm': 3.907128095626831, 'learning_rate': 8.689407540394975e-06, 'epoch': 113.11}\n",
      "{'loss': 0.0688, 'grad_norm': 3.8891761302948, 'learning_rate': 8.599640933572712e-06, 'epoch': 114.0}\n",
      "{'loss': 0.0672, 'grad_norm': 2.729755163192749, 'learning_rate': 8.509874326750449e-06, 'epoch': 114.9}\n",
      "{'loss': 0.0668, 'grad_norm': 4.688126087188721, 'learning_rate': 8.420107719928188e-06, 'epoch': 115.8}\n",
      "{'loss': 0.0662, 'grad_norm': 3.630549907684326, 'learning_rate': 8.330341113105925e-06, 'epoch': 116.7}\n",
      "{'loss': 0.0659, 'grad_norm': 3.252448081970215, 'learning_rate': 8.240574506283662e-06, 'epoch': 117.59}\n",
      "{'loss': 0.0654, 'grad_norm': 3.7045814990997314, 'learning_rate': 8.1508078994614e-06, 'epoch': 118.49}\n",
      "{'loss': 0.0654, 'grad_norm': 3.273299217224121, 'learning_rate': 8.06104129263914e-06, 'epoch': 119.39}\n",
      "{'loss': 0.0648, 'grad_norm': 2.218808889389038, 'learning_rate': 7.971274685816877e-06, 'epoch': 120.29}\n",
      "{'loss': 0.0648, 'grad_norm': 4.44417667388916, 'learning_rate': 7.881508078994614e-06, 'epoch': 121.18}\n",
      "{'loss': 0.0642, 'grad_norm': 2.9611175060272217, 'learning_rate': 7.791741472172353e-06, 'epoch': 122.08}\n",
      "{'loss': 0.0638, 'grad_norm': 4.328582763671875, 'learning_rate': 7.70197486535009e-06, 'epoch': 122.98}\n",
      "{'loss': 0.0629, 'grad_norm': 2.4155523777008057, 'learning_rate': 7.6122082585278276e-06, 'epoch': 123.88}\n",
      "{'loss': 0.0632, 'grad_norm': 3.4053096771240234, 'learning_rate': 7.522441651705566e-06, 'epoch': 124.78}\n",
      "{'loss': 0.0622, 'grad_norm': 2.5449705123901367, 'learning_rate': 7.432675044883304e-06, 'epoch': 125.67}\n",
      "{'loss': 0.0623, 'grad_norm': 3.291341543197632, 'learning_rate': 7.342908438061041e-06, 'epoch': 126.57}\n",
      "{'loss': 0.0615, 'grad_norm': 3.333635091781616, 'learning_rate': 7.25314183123878e-06, 'epoch': 127.47}\n",
      "{'loss': 0.0614, 'grad_norm': 3.405346155166626, 'learning_rate': 7.163375224416518e-06, 'epoch': 128.37}\n",
      "{'loss': 0.0612, 'grad_norm': 2.3251960277557373, 'learning_rate': 7.073608617594255e-06, 'epoch': 129.26}\n",
      "{'loss': 0.0609, 'grad_norm': 2.1835761070251465, 'learning_rate': 6.983842010771993e-06, 'epoch': 130.16}\n",
      "{'loss': 0.0611, 'grad_norm': 2.926089286804199, 'learning_rate': 6.894075403949732e-06, 'epoch': 131.06}\n",
      "{'loss': 0.0605, 'grad_norm': 3.640115261077881, 'learning_rate': 6.804308797127469e-06, 'epoch': 131.96}\n",
      "{'loss': 0.0598, 'grad_norm': 2.6534013748168945, 'learning_rate': 6.714542190305207e-06, 'epoch': 132.85}\n",
      "{'loss': 0.0599, 'grad_norm': 3.463719129562378, 'learning_rate': 6.624775583482945e-06, 'epoch': 133.75}\n",
      "{'loss': 0.0591, 'grad_norm': 2.4438395500183105, 'learning_rate': 6.535008976660683e-06, 'epoch': 134.65}\n",
      "{'loss': 0.0593, 'grad_norm': 2.1358470916748047, 'learning_rate': 6.445242369838421e-06, 'epoch': 135.55}\n",
      "{'loss': 0.0587, 'grad_norm': 2.8474032878875732, 'learning_rate': 6.355475763016159e-06, 'epoch': 136.45}\n",
      "{'loss': 0.0588, 'grad_norm': 2.327902317047119, 'learning_rate': 6.265709156193897e-06, 'epoch': 137.34}\n",
      "{'loss': 0.0588, 'grad_norm': 3.6000020503997803, 'learning_rate': 6.175942549371634e-06, 'epoch': 138.24}\n",
      "{'loss': 0.0578, 'grad_norm': 2.2878453731536865, 'learning_rate': 6.086175942549372e-06, 'epoch': 139.14}\n",
      "{'loss': 0.058, 'grad_norm': 2.977485179901123, 'learning_rate': 5.99640933572711e-06, 'epoch': 140.04}\n",
      "{'loss': 0.0576, 'grad_norm': 3.5180232524871826, 'learning_rate': 5.9066427289048475e-06, 'epoch': 140.93}\n",
      "{'loss': 0.0569, 'grad_norm': 2.7011942863464355, 'learning_rate': 5.8168761220825854e-06, 'epoch': 141.83}\n",
      "{'loss': 0.0568, 'grad_norm': 2.9322047233581543, 'learning_rate': 5.727109515260324e-06, 'epoch': 142.73}\n",
      "{'loss': 0.0571, 'grad_norm': 2.804605484008789, 'learning_rate': 5.637342908438061e-06, 'epoch': 143.63}\n",
      "{'loss': 0.0562, 'grad_norm': 2.281008005142212, 'learning_rate': 5.547576301615799e-06, 'epoch': 144.52}\n",
      "{'loss': 0.056, 'grad_norm': 2.4153645038604736, 'learning_rate': 5.457809694793538e-06, 'epoch': 145.42}\n",
      "{'loss': 0.0558, 'grad_norm': 2.1009202003479004, 'learning_rate': 5.368043087971275e-06, 'epoch': 146.32}\n",
      "{'loss': 0.0558, 'grad_norm': 3.9654667377471924, 'learning_rate': 5.278276481149013e-06, 'epoch': 147.22}\n",
      "{'loss': 0.0557, 'grad_norm': 2.359672784805298, 'learning_rate': 5.18850987432675e-06, 'epoch': 148.11}\n",
      "{'loss': 0.0557, 'grad_norm': 2.3002870082855225, 'learning_rate': 5.098743267504489e-06, 'epoch': 149.01}\n",
      "{'loss': 0.0548, 'grad_norm': 4.07916784286499, 'learning_rate': 5.008976660682227e-06, 'epoch': 149.91}\n",
      "{'loss': 0.0549, 'grad_norm': 2.7448508739471436, 'learning_rate': 4.919210053859965e-06, 'epoch': 150.81}\n",
      "{'loss': 0.0544, 'grad_norm': 2.3932082653045654, 'learning_rate': 4.829443447037702e-06, 'epoch': 151.71}\n",
      "{'loss': 0.0544, 'grad_norm': 2.8037290573120117, 'learning_rate': 4.739676840215441e-06, 'epoch': 152.6}\n",
      "{'loss': 0.054, 'grad_norm': 1.925868272781372, 'learning_rate': 4.649910233393179e-06, 'epoch': 153.5}\n",
      "{'loss': 0.0538, 'grad_norm': 1.0882643461227417, 'learning_rate': 4.560143626570916e-06, 'epoch': 154.4}\n",
      "{'loss': 0.0536, 'grad_norm': 1.4306836128234863, 'learning_rate': 4.470377019748654e-06, 'epoch': 155.3}\n",
      "{'loss': 0.0534, 'grad_norm': 1.847779393196106, 'learning_rate': 4.380610412926392e-06, 'epoch': 156.19}\n",
      "{'loss': 0.0537, 'grad_norm': 2.005582332611084, 'learning_rate': 4.2908438061041295e-06, 'epoch': 157.09}\n",
      "{'loss': 0.0533, 'grad_norm': 2.7558329105377197, 'learning_rate': 4.2010771992818675e-06, 'epoch': 157.99}\n",
      "{'loss': 0.0528, 'grad_norm': 4.412667274475098, 'learning_rate': 4.111310592459605e-06, 'epoch': 158.89}\n",
      "{'loss': 0.0527, 'grad_norm': 1.3436481952667236, 'learning_rate': 4.021543985637343e-06, 'epoch': 159.78}\n",
      "{'loss': 0.0525, 'grad_norm': 1.9011238813400269, 'learning_rate': 3.931777378815081e-06, 'epoch': 160.68}\n",
      "{'loss': 0.0521, 'grad_norm': 1.4891407489776611, 'learning_rate': 3.842010771992819e-06, 'epoch': 161.58}\n",
      "{'loss': 0.0523, 'grad_norm': 2.1668429374694824, 'learning_rate': 3.752244165170557e-06, 'epoch': 162.48}\n",
      "{'loss': 0.0519, 'grad_norm': 2.6556594371795654, 'learning_rate': 3.6624775583482946e-06, 'epoch': 163.38}\n",
      "{'loss': 0.0518, 'grad_norm': 3.3673267364501953, 'learning_rate': 3.5727109515260326e-06, 'epoch': 164.27}\n",
      "{'loss': 0.0518, 'grad_norm': 1.4020780324935913, 'learning_rate': 3.48294434470377e-06, 'epoch': 165.17}\n",
      "{'loss': 0.0516, 'grad_norm': 3.11321759223938, 'learning_rate': 3.3931777378815085e-06, 'epoch': 166.07}\n",
      "{'loss': 0.051, 'grad_norm': 1.1574417352676392, 'learning_rate': 3.3034111310592464e-06, 'epoch': 166.97}\n",
      "{'loss': 0.051, 'grad_norm': 1.6245043277740479, 'learning_rate': 3.213644524236984e-06, 'epoch': 167.86}\n",
      "{'loss': 0.0506, 'grad_norm': 2.841869354248047, 'learning_rate': 3.1238779174147223e-06, 'epoch': 168.76}\n",
      "{'loss': 0.0504, 'grad_norm': 1.4495688676834106, 'learning_rate': 3.0341113105924598e-06, 'epoch': 169.66}\n",
      "{'loss': 0.0507, 'grad_norm': 3.567607879638672, 'learning_rate': 2.9443447037701977e-06, 'epoch': 170.56}\n",
      "{'loss': 0.0502, 'grad_norm': 1.955336093902588, 'learning_rate': 2.8545780969479357e-06, 'epoch': 171.45}\n",
      "{'loss': 0.0502, 'grad_norm': 1.5435457229614258, 'learning_rate': 2.7648114901256736e-06, 'epoch': 172.35}\n",
      "{'loss': 0.05, 'grad_norm': 0.6705886721611023, 'learning_rate': 2.6750448833034115e-06, 'epoch': 173.25}\n",
      "{'loss': 0.05, 'grad_norm': 1.0995969772338867, 'learning_rate': 2.585278276481149e-06, 'epoch': 174.15}\n",
      "{'loss': 0.05, 'grad_norm': 1.195576786994934, 'learning_rate': 2.495511669658887e-06, 'epoch': 175.04}\n",
      "{'loss': 0.0495, 'grad_norm': 2.740884780883789, 'learning_rate': 2.405745062836625e-06, 'epoch': 175.94}\n",
      "{'loss': 0.0493, 'grad_norm': 2.359318494796753, 'learning_rate': 2.315978456014363e-06, 'epoch': 176.84}\n",
      "{'loss': 0.0494, 'grad_norm': 2.930469274520874, 'learning_rate': 2.2262118491921008e-06, 'epoch': 177.74}\n",
      "{'loss': 0.0487, 'grad_norm': 0.7461932301521301, 'learning_rate': 2.1364452423698387e-06, 'epoch': 178.64}\n",
      "{'loss': 0.0492, 'grad_norm': 0.9636931419372559, 'learning_rate': 2.0466786355475767e-06, 'epoch': 179.53}\n",
      "{'loss': 0.0485, 'grad_norm': 2.103720188140869, 'learning_rate': 1.956912028725314e-06, 'epoch': 180.43}\n",
      "{'loss': 0.0489, 'grad_norm': 1.1473742723464966, 'learning_rate': 1.8671454219030521e-06, 'epoch': 181.33}\n",
      "{'loss': 0.0488, 'grad_norm': 0.818762481212616, 'learning_rate': 1.77737881508079e-06, 'epoch': 182.23}\n",
      "{'loss': 0.0485, 'grad_norm': 0.9607476592063904, 'learning_rate': 1.687612208258528e-06, 'epoch': 183.12}\n",
      "{'loss': 0.0483, 'grad_norm': 0.6644375324249268, 'learning_rate': 1.597845601436266e-06, 'epoch': 184.02}\n",
      "{'loss': 0.0479, 'grad_norm': 0.815762996673584, 'learning_rate': 1.5080789946140036e-06, 'epoch': 184.92}\n",
      "{'loss': 0.048, 'grad_norm': 1.4025263786315918, 'learning_rate': 1.4183123877917416e-06, 'epoch': 185.82}\n",
      "{'loss': 0.0477, 'grad_norm': 2.7344467639923096, 'learning_rate': 1.3285457809694793e-06, 'epoch': 186.71}\n",
      "{'loss': 0.0476, 'grad_norm': 0.9482029676437378, 'learning_rate': 1.2387791741472172e-06, 'epoch': 187.61}\n",
      "{'loss': 0.0476, 'grad_norm': 0.9666743874549866, 'learning_rate': 1.1490125673249552e-06, 'epoch': 188.51}\n",
      "{'loss': 0.0473, 'grad_norm': 1.2198944091796875, 'learning_rate': 1.0592459605026931e-06, 'epoch': 189.41}\n",
      "{'loss': 0.0471, 'grad_norm': 0.664420485496521, 'learning_rate': 9.694793536804308e-07, 'epoch': 190.31}\n",
      "{'loss': 0.0471, 'grad_norm': 1.7937953472137451, 'learning_rate': 8.797127468581689e-07, 'epoch': 191.2}\n",
      "{'loss': 0.047, 'grad_norm': 1.692743182182312, 'learning_rate': 7.899461400359067e-07, 'epoch': 192.1}\n",
      "{'loss': 0.047, 'grad_norm': 1.836378812789917, 'learning_rate': 7.001795332136446e-07, 'epoch': 193.0}\n",
      "{'loss': 0.0466, 'grad_norm': 0.8624934554100037, 'learning_rate': 6.104129263913825e-07, 'epoch': 193.9}\n",
      "{'loss': 0.0467, 'grad_norm': 1.4421544075012207, 'learning_rate': 5.206463195691203e-07, 'epoch': 194.79}\n",
      "{'loss': 0.0466, 'grad_norm': 1.1662803888320923, 'learning_rate': 4.3087971274685824e-07, 'epoch': 195.69}\n",
      "{'loss': 0.0465, 'grad_norm': 1.2957093715667725, 'learning_rate': 3.4111310592459613e-07, 'epoch': 196.59}\n",
      "{'loss': 0.046, 'grad_norm': 0.7344472408294678, 'learning_rate': 2.5134649910233396e-07, 'epoch': 197.49}\n",
      "{'loss': 0.0462, 'grad_norm': 0.8043776154518127, 'learning_rate': 1.6157989228007181e-07, 'epoch': 198.38}\n",
      "{'loss': 0.0459, 'grad_norm': 1.94614577293396, 'learning_rate': 7.18132854578097e-08, 'epoch': 199.28}\n",
      "{'train_runtime': 12918.8094, 'train_samples_per_second': 68.969, 'train_steps_per_second': 8.623, 'train_loss': 0.2805910570017953, 'epoch': 200.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=111400, training_loss=0.2805910570017953, metrics={'train_runtime': 12918.8094, 'train_samples_per_second': 68.969, 'train_steps_per_second': 8.623, 'total_flos': 5.8202800128e+16, 'train_loss': 0.2805910570017953, 'epoch': 200.0})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model and tokenizer locally for later use without need for retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\\\\tokenizer_config.json',\n",
       " 'F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\\\\special_tokens_map.json',\n",
       " 'F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\\\\vocab.json',\n",
       " 'F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\\\\merges.txt',\n",
       " 'F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\\\\added_tokens.json',\n",
       " 'F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# robinhood_tokenizer.save_pretrained(\"F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\")\n",
    "# trainer.save_model(\"F:/Opiskelu/adaml-2024fall/week-11/robinhood_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "GPT2TokenizerFast has no attribute to",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      2\u001b[0m model_test \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/Opiskelu/adaml-2024fall/week-11/robinhood_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto( model\u001b[38;5;241m.\u001b[39mdevice )\n\u001b[1;32m----> 3\u001b[0m tokenizer_test \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m( model\u001b[38;5;241m.\u001b[39mdevice )\n",
      "File \u001b[1;32mc:\\Users\\erikk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1103\u001b[0m, in \u001b[0;36mSpecialTokensMixin.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(attr_as_tokens) \u001b[38;5;28;01mif\u001b[39;00m attr_as_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m-> 1103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(key)\n",
      "\u001b[1;31mAttributeError\u001b[0m: GPT2TokenizerFast has no attribute to"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_test = AutoModelForCausalLM.from_pretrained(\"F:/Opiskelu/adaml-2024fall/week-11/robinhood_model\")\n",
    "tokenizer_test = AutoTokenizer.from_pretrained(\"F:/Opiskelu/adaml-2024fall/week-11/robinhood_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing text generation on the trained model using beam search. Reference: https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Little John was sus, no cap.  There is no so far Little John and six to Little John and Will Stutely cameleaping and stood upon the stope, relad thime thim whilllly that t thathe d no be mear bext t t\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Little John was sus, no cap.\"\n",
    "\n",
    "input_ids = robinhood_tokenizer.encode( input_text, return_tensors=\"pt\" ).to( model.device )\n",
    "\n",
    "greedy_output_ids = model.generate(\n",
    "    input_ids,  \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "greedy_output = robinhood_tokenizer.decode( greedy_output_ids[0], skip_special_tokens=False )\n",
    "\n",
    "print( \"\\n\", greedy_output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Little John was sus, no cap.  But for the two stout yeoman in Lincolngreen green away through the others were never could belonginging to the the r, this hatror had ppayoush roff t trof trof thenof t \n"
     ]
    }
   ],
   "source": [
    "input_text = \"Little John was sus, no cap.\"\n",
    "\n",
    "input_ids_test = tokenizer_test.encode( input_text, return_tensors=\"pt\" ).to('cuda')\n",
    "\n",
    "greedy_output_ids = model_test.generate(\n",
    "    input_ids_test,  \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "greedy_output = tokenizer_test.decode( greedy_output_ids[0], skip_special_tokens=False )\n",
    "\n",
    "print( \"\\n\", greedy_output )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs: Huge impact on computational time; block size: huge impact on comp time; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb\n",
    "\n",
    "https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb\n",
    "\n",
    "Additionally, Hugging Face's documentation on Transformers, Datasets and Tokenizers:\n",
    "\n",
    "https://huggingface.co/docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
